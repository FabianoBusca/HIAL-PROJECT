{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrOtTxgMeTEM"
   },
   "source": [
    "- title: Imitation Learning\n",
    "- summary: step by step practical about BC\n",
    "- author: Silvia TULLI\n",
    "- feedback and revision: Kim BARAKA, Mohamed CHETOUANI, Muhan HOU\n",
    "- teaching assistant: Márton Bodó, Fabiano Busca\n",
    "- date: 2026-February-March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrLYYqV_DTZy"
   },
   "source": [
    "This notebook contains an excerpt from the **Human-Interactive Robot Learning (HIRL)** educational module.\\\n",
    "For more information check out [our website](https://sites.google.com/view/hirl-education?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGSV7aUaAjHs"
   },
   "source": [
    "The practical can be done alone or with a colleague. Please add below your information.\n",
    "\n",
    "Student(s):\\\n",
    "(1) NAME___________________ SURNAME___________________ ID___________________ Course___________________\\\n",
    "(2) NAME___________________ SURNAME___________________ ID___________________ Course___________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLrxTZc1ST32"
   },
   "source": [
    "# **Goals**\n",
    "* Understand and apply Imitation Learning (IL) techniques to simple tasks.\n",
    "* Implement direct Behavioral Cloning (BC) algorithm.\n",
    "* Experiment with parameters such as the number of demonstrations to analyze their impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elicxP-GMyPS"
   },
   "source": [
    "# **Imitation learning**\n",
    "\n",
    "Imitation Learning (IL) is a set of techniques aimed at training a model to directly mimic an expert's actions from a collection of demonstrations. In this practical, we focus on **offline imitation learning**, meaning we assume there exists a dataset consisting of demonstration data in which each sample correspond to a state-action pair collected from the expert. In practice, this might mean collecting keystrokes of a human player on an Atari game, controls of a human-driven car, or medical decisions of a doctor along a course of treatment. In this case, the goal of an IL algorithm is to learn a policy (i.e., a full mapping from states to actions) that mimics the expert policy (for which you only have a set of observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUuXhR-mTC__"
   },
   "source": [
    "# **Behavioral Cloning (BC)**\n",
    "\n",
    "Behavioral Cloning is a simple but effective imitation learning technique that involves training a model by learning a direct mapping from states to actions, without any intermediate representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAd5gKc-Adxj"
   },
   "source": [
    "We're tackling imitation learning within a specific environment characterized by a discrete Markov Decision Process (MDP) with a fixed time horizon of $T$. There is an expert policy, denoted as $\\pi^*$, which provides deterministic actions at each state.\n",
    "\n",
    "The input of a Behavioral Cloning algorithm is a restricted policy class $\\Pi=\\{\\pi: S \\mapsto \\Delta(A)\\}$\n",
    "\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\pi_\\theta =\\arg \\min _{\\pi \\in \\Pi} \\sum_{i=1}^{M} \\ell\\left(\\pi, s^{\\star}, a^{\\star}\\right) \\\\\n",
    "\\text { loss function }\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We consider a set of human expert trajectories that adhere to $\\pi$ and we aim to create an imitation policy, $\\pi_\\theta$, that replicates these expert trajectories effectively. The objective is to ensure that for each state, the action chosen by our policy $\\pi_θ$ is exactly the same as the action chosen by the expert policy $\\pi$.\n",
    "\n",
    "In simpler terms, we aim to develop a policy that perfectly imitates the expert's actions in the given environment, making deterministic choices at each state to mimic the expert's behavior precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW3ryRBbFYy3"
   },
   "source": [
    "# **Environment Set up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxqXIoMOXcoi"
   },
   "source": [
    "First, we define the environment, in this case a 5x5 gridworld. We also define the transition probabilities for each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZmB3Eamq7f0"
   },
   "source": [
    "## Environment 1 definition\n",
    "\n",
    "The environment looks like this:\\\n",
    "![grid](https://drive.google.com/uc?id=1cq-2fJ3pRMqDJp179xShCR3TsYBue_TV)\n",
    "![action space](https://drive.google.com/uc?id=1UogrQp_KsuXPHl70RWcpe8BVfQTXlY32)\n",
    " ![actions](https://drive.google.com/uc?id=1OlXjbkEWVp-VSzc-RZ1B6OQXLOrTAFGI)\\\n",
    " Each state is represented by a number.\n",
    " The goal is to reach the `State 25` (i.e., flag cell) from any random state. There are three obstacles (i.e., TNT cells). The four actions that the agent can execute are: Up - Down - Left - Right. The agent cannot cross walls; therefore, the action space looks as pictured above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1kYTHkoHDtz"
   },
   "source": [
    "The transition matrix defines an MDP (Markov Decision Process) representing a system's dynamics and the impact of a specific action, labeled as `Action Up`. The matrix's rows correspond to distinct states within the MDP, ranging from state 1 (cell 0) to state 25 (cell 24). Each column represents a potential subsequent state that the system might transition to when `Action Up` is taken.\n",
    "\n",
    "Example of a transition matrix for Action Up (0)\n",
    "```\n",
    "#               1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25                \n",
    "Pu = np.array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 1\n",
    "               [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 2\n",
    "               [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 3\n",
    "               [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 4\n",
    "               [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 5\n",
    "               ...\n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "In the given context:\n",
    "\n",
    "The rows symbolize the current state of the system within the MDP, comprising $25$ distinct states.\n",
    "The columns represent the feasible future states that the system can transition to when `Action Up` is executed.\n",
    "The values in the matrix signify the probabilities of transitioning from the current state (row) to a specific subsequent state (column) upon taking `Action Up`.\n",
    "\n",
    "For instance, examining the first row `State 1`, it demonstrates a probability of $1.0$ (or $100$%) of transitioning to `State 1` when `Action Up` is performed. This implies that if the system is presently in `State 1` and `Action Up` is executed, it will unquestionably move to `State 1`.\n",
    "\n",
    "Conversely, examining the first column provides insights into the probabilities of transitioning to different states from any initial state when `Action Up` is employed. For `Action Up`, the system is more inclined to remain in the same state (diagonal elements are 1.0) and less likely to transition to other states (off-diagonal elements are 0.0).\n",
    "\n",
    "It's important to note that this MDP describes the system's probabilistic behavior in response to `Action Up` and is distinct from a standard Markov chain, as it incorporates the notion of making decisions (in this case, taking an action) and observing the subsequent state transitions.\n",
    "\n",
    "Below an example representation of an MDP from wikipedia:\\\n",
    "![wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/400px-Markov_Decision_Process.svg.png).\\\n",
    "\n",
    "Here you can observe that from $S0$ by performing $a0$ there is 50% of probability of staying in $S0$ and 50% of probability of going to $S2$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn7e2d-zNOT3"
   },
   "source": [
    "### **Question 1**\n",
    "\n",
    "* Fill in the blank spaces (i.e., the function of reset and step) with your code to specify the deterministic environment dynamics of the 5x5 gridworld defined above (not the example figure of MDP from wikipedia).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CNlawwSKiriT"
   },
   "source": [
    "# GridWorldEnvironment class\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, grid_size: Tuple[int, int] = (5, 5), obstacles: List[int] = []) -> None:\n",
    "        self.grid_size = grid_size\n",
    "        self.state_space = np.prod(grid_size)\n",
    "        self.action_space = 4  # Four possible actions: Up, Down, Left, Right\n",
    "        self.state: Optional[int] = None\n",
    "        self.obstacles = obstacles\n",
    "\n",
    "    def reset(self) -> Optional[int]:\n",
    "        # Randomly select an initial state that is not an obstacle\n",
    "        ''' Add your code here '''\n",
    "\n",
    "\n",
    "    def step(self, action: int) -> Tuple[int, int, bool]:\n",
    "        if self.state is None:\n",
    "            raise Exception(\"You must call reset() before step()\")\n",
    "\n",
    "        # Define transitions for each action (move in the grid)\n",
    "        ''' Add your code here '''\n",
    "        next_state =\n",
    "\n",
    "        # Update the state if the next state is not an obstacle\n",
    "        if next_state not in self.obstacles:\n",
    "            self.state = next_state\n",
    "        else:\n",
    "            return self.state, -1, False  # Negative reward for hitting an obstacle\n",
    "\n",
    "        # Define the reward function (e.g., reaching a goal state)\n",
    "        done = (self.state == self.state_space - 1)  # Goal state\n",
    "        reward = 1 if done else 0\n",
    "\n",
    "        return self.state, reward, done"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5OVIkCUN_3H"
   },
   "source": [
    "# **Dataset Generation**\n",
    "Then we generate our dataset of expert demonstrations, which are represented as a state-action pairs $(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J37wyP6cYMp8"
   },
   "source": [
    "### **Question 2**\n",
    "\n",
    "* Fill in the blank spaces (i.e., the function of simple_policy) with your code to specify a simple policy that avoids obstacles and follows a right-down strategy (i.e., first moving right until hit the wall, then moving down to reach the goal, or moving left if neither moving right or down is possible in the cases of obstalces)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UmNy6wey0P6b"
   },
   "source": [
    "# Generate expert demonstrations\n",
    "def generate_expert_demonstrations(env: GridWorldEnvironment, num_demos: int) -> List[List[Tuple[int, int]]]:\n",
    "    expert_demonstrations = []\n",
    "\n",
    "    for _ in range(num_demos):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "\n",
    "        while state != env.state_space - 1:  # Continue until reaching the goal state\n",
    "            # Define a policy to reach the goal state while avoiding obstacles\n",
    "            action = simple_policy(env, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            trajectory.append((state, action))\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        expert_demonstrations.append(trajectory)\n",
    "\n",
    "    return expert_demonstrations\n",
    "\n",
    "def simple_policy(env: GridWorldEnvironment, state: int) -> int:\n",
    "    # A simple policy that avoids obstacles and follows a right-down strategy.\n",
    "    ''' Add your code here '''\n",
    "\n",
    "    return action\n",
    "\n",
    "# Create the grid world environment\n",
    "env = GridWorldEnvironment(obstacles=[7, 8, 12])\n",
    "\n",
    "# Generate expert demonstrations\n",
    "num_demos = 10  # You can adjust the number of demonstrations\n",
    "expert_demonstrations = generate_expert_demonstrations(env, num_demos)\n",
    "\n",
    "# Print the expert demonstrations\n",
    "for idx, demo in enumerate(expert_demonstrations):\n",
    "    print(f\"Expert Demonstration {idx + 1}: {demo}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbr-GrNKPIwO"
   },
   "source": [
    "In this example the expert agent attempts to move right when it's possible (i.e., not in the last column), and when it reaches the last column, it moves down. This policy is designed to guide the agent toward the goal state located in the bottom-right corner of the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01q8FiazX-jM"
   },
   "source": [
    "Below we can observe that the demonstrations dataset comprises of state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZC8g9FwdX6t8"
   },
   "source": [
    "expert_demonstrations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfhb8EVrPO82"
   },
   "source": [
    "### **Question 3**\n",
    "\n",
    "In the code above, we collected demonstrations that were generated by a rule-based simple policy as they were given from a human.\n",
    "\n",
    "* Can you come up with any other alternative demonstrations for this environment (e.g., based on an alternative policy)?\n",
    "* Can you suggest any other ways to generate these demonstrations (as opposed to being generated by a predefined policy)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQ7JMbafG5L"
   },
   "source": [
    "### **Your answers to Question 3**\n",
    "\n",
    "* Q3.1:\n",
    "\n",
    "* Q3.2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf4YrKzVTg6Z"
   },
   "source": [
    "# **Define the Behavioral Cloning Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2384vlsJTau4"
   },
   "source": [
    "We set up the Python environment and import necessary libraries, including TensorFlow or PyTorch, depending on your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfj1ficYT16C"
   },
   "source": [
    "# Model Structure\n",
    "The BehavioralCloningModel described below consists of a Neural Network with three fully connected (dense) layers:\\\n",
    "Input (1D) -> [fc1] -> [ReLU] -> [fc2] -> [ReLU] -> [fc3] -> Output (Mapping from states to deterministic actions)\n",
    "* `Input (1D)` represents the input layer with a single node, as state_dim is 1.\n",
    "* `[fc1]` represents the first fully connected layer with 64 neurons.\n",
    "* `[ReLU]` represents the Rectified Linear Unit (ReLU) activation function applied after each fully connected layer.\n",
    "* `[fc2]` represents the second fully connected layer with 64 neurons.\n",
    "* `[fc3]` represents the third fully connected layer, which outputs the mapping from states to deterministic actions. The number of neurons in this layer is determined by action_dim.\n",
    "\n",
    "The arrows between layers represent the connections and transformations of data as it flows through the network during forward pass. The ReLU activation functions introduce non-linearity in the model.\n",
    "\n",
    "The model learns a policy using **supervised learning** by minimizing the loss function. In behavioral cloning, the model is trained to predict actions that are as close as possible to the actions taken by the expert. Accuracy is then used to evaluate the model.\n",
    "\n",
    "In the code below, the **cross-entropy loss** quantifies the dissimilarity between the model's predicted action and the true actions from the expert.\n",
    "\n",
    "In the context of behavioral cloning for deterministic environments (where expert actions are treated as deterministic), the cross-entropy loss is used to train the model to predict actions that resemble the expert's actions. However, it's important to note that this approach assumes that the expert's actions are ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJpxUSwPFdPL"
   },
   "source": [
    "Find out more about cross-entropy loss in [3.2.1.3 Log Loss Function](https://takaosa.github.io/paper/algorithmic-perspective-imitation.pdf).\n",
    "A general overview of imitation learning approaches, including different loss functions can be found in [Imitation Learning Lecture](https://web.stanford.edu/class/cs237b/pdfs/lecture/lecture_10111213.pdf) from Stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3oF0d052Khl"
   },
   "source": [
    "![architecture](https://drive.google.com/uc?id=14NuVh8bLTX9Og1peV3y7_1yygCTO0i4I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBU9dsNLrJMu"
   },
   "source": [
    "### **Question 4**\n",
    "* Fill in the blank spaces (i.e., in the function of train) with your code to calculate accuracy for the current epoch and store it. You will use your saved results for plotting in the next question."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HyDS0cTzYWq-"
   },
   "source": [
    "# Define the Behavioral Cloning Model\n",
    "class BehavioralCloningModel(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int) -> None:\n",
    "        super(BehavioralCloningModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, states: List[List[int]], actions: List[int], epochs: int = 50, batch_size: int = 32) -> Tuple[List[float], List[float], float, float]:\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        print('states', states)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(states, actions)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        accuracy_values = []\n",
    "        loss_values = []  # Initialize loss_values to track loss values\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            for batch_states, batch_actions in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                predicted_actions = model(batch_states)\n",
    "                loss = loss_fn(predicted_actions, batch_actions)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate the number of correct predictions in the current batch\n",
    "                correct_predictions += (predicted_actions.argmax(dim=1) == batch_actions).sum().item()\n",
    "\n",
    "            # Calculate accuracy for the current epoch and store it\n",
    "            ''' Add your code here '''\n",
    "\n",
    "            # Append the average loss for the current epoch to loss_values\n",
    "            loss_values.append(total_loss / len(dataloader))\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss_values[-1]:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "        # Calculate standard deviation for loss and accuracy\n",
    "        loss_std = np.std(loss_values)\n",
    "        accuracy_std = np.std(accuracy_values)\n",
    "\n",
    "        return loss_values, accuracy_values, loss_std, accuracy_std # Return loss and accuracy values\n",
    "\n",
    "# Extract states and actions from expert demonstrations\n",
    "states = []\n",
    "actions = []\n",
    "\n",
    "for demonstration in expert_demonstrations:\n",
    "    for state, action in demonstration:\n",
    "        states.append([state])  # Wrap the state in a list to make it 2D (1x1)\n",
    "        actions.append(action)\n",
    "\n",
    "# Define state_dim and action_dim based on your data\n",
    "state_dim = 1  # Assuming state is a scalar value\n",
    "action_dim = max(actions) + 1  # Calculate action_dim based on the maximum action value\n",
    "\n",
    "# Instantiate the model\n",
    "model = BehavioralCloningModel(state_dim, action_dim)\n",
    "\n",
    "# Train the model with the expert demonstration data\n",
    "loss_values, accuracy_values, loss_std, accuracy_std = model.train(states, actions, epochs=500, batch_size=32)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHryxFPHcb8N"
   },
   "source": [
    "### **Question 5**\n",
    "* Fill in the blank spaces with your code to plot the training results of loss and accuracy respectively (i.e., two figures in total). For each figure, use the epoch number as the x-axis and the corresponding measurement (either loss or accuracy) as the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vcIRUIqmsCCL"
   },
   "source": [
    "epochs = 500\n",
    "# Define lighter colors for the standard deviation lines\n",
    "lighter_blue = \"b\"\n",
    "lighter_red = \"r\"\n",
    "\n",
    "# Plot Loss and Accuracy\n",
    "''' Add you code here'''\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "djqnEAzLtZj3"
   },
   "source": [
    "# Visualize the Distribution of Actions\n",
    "action_counts = [actions.count(i) for i in range(action_dim)]\n",
    "plt.bar(range(action_dim), action_counts)\n",
    "plt.xticks(range(action_dim))\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Actions\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6LjbBJCjavV"
   },
   "source": [
    "Note that while the term *probabilities* might imply a probabilistic policy, in this deterministic case, these values represent the model's confidence or preference for each possible action given a particular state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSbzgQIFeBA_"
   },
   "source": [
    "# **Submission of your assignment**\n",
    "\n",
    "* submit your completed colab file\n",
    "\n",
    "* save and submit your two figures plotted in Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHkFWcvrDI9o"
   },
   "source": [
    "# **References**\n",
    "* D. A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3:88–97, 1991.\n",
    "* F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. ArXiv, abs/1805.01954, 2018.\n",
    "* [Imitation Learning open source library](https://imitation.readthedocs.io/en/latest/algorithms/bc.html)\n",
    "* CS 285 at UC Berkeley, [Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/)\n",
    "* M. Chetouani. Interactive Robot Learning: An Overview. Chetouani, M.; Dignum, V.; Lukow- icz, P.; Sierra, C. Human-Centered Artificial Intelligence, 13500, Springer International Publishing, pp.140-172, 2023, Lecture Notes in Computer Science, 10.1007/978-3-031-24349-3_9 . [hal-04060804](https://hal.science/hal-04060804/file/ACAI2021_chetouani_author-version.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
