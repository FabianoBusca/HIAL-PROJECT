{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Silviatulli/HIRL-education/blob/main/Learning_from_Feedback_Student_Version_Tetris_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqGnwug1CwAs"
   },
   "source": [
    "- title: Learning from Human Feedback\n",
    "- summary: step by step tutorial about TAMER\n",
    "- authors: Silvia TULLI, Mark DE BRUIJN, Márton Bodó\n",
    "- feedback and revision: Kim BARAKA, Mohamed CHETOUANI, Muhan HOU\n",
    "- teaching assistant: Márton Bodó, Fabiano Busca\n",
    "- date: 2026-February-March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6hqqBl6C5oV"
   },
   "source": [
    "This notebook contains an excerpt from the **Human-Interactive Robot Learning (HIRL)** educational module.\\\n",
    "For more information check out [our website](https://sites.google.com/view/hirl-education?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3XpSIRzD-rb"
   },
   "source": [
    "The practical can be done alone or with a colleague. Please add below your information.\n",
    "\n",
    "Student(s):\\\n",
    "(1) NAME___________________ SURNAME___________________ ID___________________ Course___________________\\\n",
    "(2) NAME___________________ SURNAME___________________ ID___________________ Course___________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuV1gtoKEIro"
   },
   "source": [
    "# **Goals**\n",
    "* Familiarize with [Gymnasium](https://gymnasium.farama.org/) environments.\n",
    "* Apply Learning from Human Feedback techniques to simple tasks.\n",
    "* Implement TAMER algorithm and a more advanced Learning from Human Feedback algorithm.\n",
    "* Experiment with parameters such as the number of feedback and their modality to analyze their impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opCZ-vTrEaFe"
   },
   "source": [
    "# **Prerequisites**\n",
    "\n",
    "* Mathematics for machine learning: linear algebra, calculus, probability and statitics\n",
    "* Python programming for data science\n",
    "* Familiarity with the lecture on interactive robot learning, in particular read the chapter [Interactive Robot Learning](https://hal.science/hal-04060804/file/ACAI2021_chetouani_author-version.pdf)\n",
    "\n",
    "To better understand the TAMER algorithm, it is necessary to take a step back and review the basics of Reinforcement Learning. A comprehensive reference on Reinforcement Learning is [Barto & Sutton (2018)](https://drive.google.com/file/d/14Ffi1SsuqoFEW87zqX3AE2oeMdQ5SGDZ/view?usp=sharing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk1nbk-kEdWG"
   },
   "source": [
    "# **Training an Agent Manually via Evaluative Reinforcement**\n",
    "\n",
    "When demonstrations are challenging for humans to provide, evaluative feedback—evaluating the quality of an action in a given state—can be more intuitive. The TAMER (Training an Agent Manually via Evaluative Reinforcement) algorithm merges elements of imitation learning and evaluative reinforcement learning, addressing the challenges of providing demonstrations. There are several versions of the TAMER algorithm. The **initial version relies solely on rewards from a human trainer**, who receives information about the current state, typically through a visual representation. In subsequent versions, the agent also learns autonomously, **incorporating both the trainer's feedback and environmental rewards by integrating with an existing reinforcement learning** (RL) algorithm. In this assingment, **you will implement a simplified subsequent version of TAMER (i.e., TAMER + RL) based on [this paper](https://bradknox.net/public/papers/aamas12-knox.pdf)**.\n",
    "\n",
    "TAMER differs from traditional imitation learning in that it does not require a fully labeled dataset of expert demonstrations. Instead, it leverages ongoing feedback from a human during the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYJc5QOQc7vI"
   },
   "source": [
    "The core steps of the TAMER algorithm are as follows:\n",
    "1. Initialize the agent's policy.\n",
    "2. Interact with the environment using this policy.\n",
    "3. Collect human feedback on the agent's actions.\n",
    "4. Update the agent's policy using the feedback.\n",
    "5. Repeat steps 2-4 until the agent's performance improves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hupAp9-FHzy4"
   },
   "source": [
    "# **Environment Set Up**\n",
    "\n",
    "To evaluate our algorithms, we will utilize benchmark environments provided by [Gymnasium](https://gymnasium.farama.org). First, we install and import the necessary libraries.\n",
    "\n",
    "* Gymnasium is an open-source platform maintained by the Farama Foundation (originally developed by OpenAI) to provide a standardized environment for testing and developing reinforcement learning algorithms.\n",
    "\n",
    "* matplotlib is used to render the environment.\n",
    "\n",
    "* Scikit-Learn will be used to build our Human Reward Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OenRYO8QEcXS",
    "outputId": "0fda454b-474d-4807-f64a-8735afae5762"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQn2zv7ZEZfX"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import update_display, clear_output\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from typing import Dict, Tuple\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "# Human Reward Model\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5SVpPNRSFMT"
   },
   "source": [
    "## Cart Pole environment\n",
    "\n",
    "In the foundational work by [Knox and Stone 2008](https://www.cs.utexas.edu/~bradknox/papers/icdl08-knox.pdf), the authors utilized Tetris, from the [RL-library](https://code.google.com/archive/p/rl-library/), to demonstrate the TAMER framework. For this assignment, you will implement a hybrid version (TAMER + RL) based on [this paper](https://bradknox.net/public/papers/aamas12-knox.pdf), applied to the Gymnasium CartPole environment.\n",
    "\n",
    "In CartPole, a pole is attached to a cart moving along a frictionless track. The goal is to apply forces of +1 or 0 to keep the pole upright. We chose this environment because it allows for intuitive human evaluation: a human can easily judge if a \"push\" was appropriate given the pole's tilt, whereas momentum-based tasks like Mountain Car are often harder for humans to critique in real-time.\n",
    "\n",
    "**State and Action Space**\n",
    "\n",
    "- Observation Space: A continuous 4D vector: `[Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]`.\n",
    "\n",
    "**The Model vs. The Agent:**\n",
    "\n",
    "- The Agent (Q-Learning): To keep the assignment computationally light, the agent's Q-table is simplified by discretizing the Pole Angle and rounding the Angular Velocity.\n",
    "\n",
    "- The Human Model (Decision Tree): Unlike the Q-table, the Human Reward Model uses the full continuous state space. This allows the agent to generalize the user's feedback more effectively than simple table-based methods.\n",
    "\n",
    "**Action Space:**\n",
    "\n",
    "- 0: Push cart to the left\n",
    "\n",
    "- 1: Push cart to the right\n",
    "\n",
    "In this interactive setup, the agent will propose an action, and you will provide evaluative feedback. The agent doesn't just store your response; it learns a model of \"you\" to predict how you would reward its actions in states where you remain silent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfd_Q_uB30hh"
   },
   "source": [
    "## **Question 1 (TAMER + RL)**\n",
    "\n",
    "In this section, you will implement the first hybrid approach: **Reward Shaping**. According to the TAMER+RL framework, the agent learns from a combined reward signal `R_total` (Note: section 3.1 of the paper of TAMER + RL could be helpful)\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Complete the update_q_values method using the SARSA update rule.\n",
    "\n",
    "2. In the training loop, define total_reward by combining the environment reward and the human model's prediction using the scaling factor β."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The human Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a Human Reward Model using a `DecisionTreeRegressor` from the `sklearn` library. Instead of using raw human input at every step, the agent trains this model to generalize the human's preferences across the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanModel:\n",
    "    def __init__(self):\n",
    "        # We use a Decision Tree to map [state, action] -> predicted_human_reward\n",
    "        self.model = DecisionTreeRegressor(max_depth=5)\n",
    "        self.X = []  # State-Action pairs\n",
    "        self.y = []  # Human feedback labels\n",
    "        self.fitted = False\n",
    "\n",
    "    def train(self, state, action, feedback):\n",
    "        # Store the interaction\n",
    "        features = np.append(state, action)\n",
    "        self.X.append(features)\n",
    "        self.y.append(feedback)\n",
    "\n",
    "        # Re-fit the model once we have enough samples\n",
    "        if len(self.X) % 5 == 0 and len(self.X) >= 5:\n",
    "            self.model.fit(self.X, self.y)\n",
    "            self.fitted = True\n",
    "\n",
    "    def predict(self, state, action):\n",
    "        if not self.fitted:\n",
    "            return 0.0\n",
    "        features = np.append(state, action).reshape(1, -1)\n",
    "        return self.model.predict(features)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Environment Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we initialize the environment and define state and action metadata.  \n",
    "Furthermore, we created a helper function to extract the velocity sign from the cartpole state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBJbnIVsWyEU"
   },
   "outputs": [],
   "source": [
    "# Initialize Environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "# Define state and action metadata\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "actions_dic = {0: 'Push Left', 1: 'Push Right'}\n",
    "\n",
    "\n",
    "def extract_velocity(value: float) -> int:\n",
    "    \"\"\"Helper to discretize velocity into 0 (Left) or 1 (Right)\"\"\"\n",
    "    return 1 if value > 0 else 0\n",
    "\n",
    "\n",
    "print(f\"Environment Initialized. State Space: {state_size}, Action Space: {action_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The TAMER Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined the TAMER agent, your **task is to complete the update_q_value method** by using the provided alpha, beta, and gamma hyperparameters. In other words, implement the SARSA update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAMERAgent:\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_values: Dict[Tuple[float, int], Dict[int, float]] = {}\n",
    "\n",
    "    def act(self, state: np.ndarray, rounding_size: int) -> int:\n",
    "        state_key = (np.round(state[2], rounding_size), extract_velocity(state[3]))\n",
    "        \n",
    "        # Initialize if unseen\n",
    "        if state_key not in self.q_values:\n",
    "            self.q_values[state_key] = {a: 0.0 for a in range(self.action_size)}\n",
    "\n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < 0.1:  \n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            return max(self.q_values[state_key], key=self.q_values[state_key].get)\n",
    "\n",
    "    def update_q_values(self, state: np.ndarray, action: int, total_reward: float, \n",
    "                        next_state: np.ndarray, next_action: int, \n",
    "                        alpha: float, gamma: float, rounding_size: int, done: bool) -> None:\n",
    "        \n",
    "        state_key = (np.round(state[2], rounding_size), extract_velocity(state[3]))\n",
    "        next_state_key = (np.round(next_state[2], rounding_size), extract_velocity(next_state[3]))\n",
    "\n",
    "        # Ensure keys exist\n",
    "        if state_key not in self.q_values:\n",
    "            self.q_values[state_key] = {a: 0.0 for a in range(self.action_size)}\n",
    "        if next_state_key not in self.q_values:\n",
    "            self.q_values[next_state_key] = {a: 0.0 for a in range(self.action_size)}\n",
    "\n",
    "        # QUESTION: Implement the update rule\n",
    "        # Use the provided alpha, gamma, and total_reward\n",
    "        ''' Add your code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Interactive Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the TAMER agent. \n",
    "\n",
    "**Task**: You have to calculate the shaped `total_reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha, beta, gamma = 0.1, 5.0, 0.95\n",
    "num_episodes, timeout_threshold = 50, 500\n",
    "feedback_budget, rounding_size = 50, 1\n",
    "\n",
    "# Reset metrics\n",
    "tamer_agent = TAMERAgent(state_size, action_size)\n",
    "human_model = HumanModel()\n",
    "total_reward_per_episode = []\n",
    "\n",
    "# Plot\n",
    "fig, ax_sim = plt.subplots(figsize=(6, 4))\n",
    "display_handle = display(fig, display_id='simulation_window')\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = np.array(obs)\n",
    "    episode_reward = 0\n",
    "    feedback_count = 0\n",
    "    \n",
    "    # Initial Action\n",
    "    action = tamer_agent.act(state, rounding_size)\n",
    "\n",
    "    for iteration in range(timeout_threshold):\n",
    "        \n",
    "        # Render simulation\n",
    "        image = env.render()\n",
    "        ax_sim.clear()\n",
    "        ax_sim.imshow(image)\n",
    "        ax_sim.set_title(f\"Episode: {episode+1} | Step: {iteration}\\n\"\n",
    "                        f\"Agent proposes: {actions_dic[action]}\")\n",
    "        ax_sim.axis('off')\n",
    "        update_display(fig, display_id='simulation_window')\n",
    "\n",
    "        # Interective Human Feedback\n",
    "        if feedback_count < feedback_budget and random.random() > 0.85:\n",
    "            clear_output(wait=True) \n",
    "            display(fig, display_id='simulation_window')\n",
    "            \n",
    "            print(f\"--- HUMAN EVALUATION (Episode {episode+1}) ---\")\n",
    "            print(f\"Agent proposes: {actions_dic[action]}. Is this good? (1: Yes, 0: No)\")\n",
    "            try:\n",
    "                user_input = input(\"Feedback (or press Enter to skip): \").strip()\n",
    "                if user_input in ['0', '1']:\n",
    "                    raw_feedback = float(user_input)\n",
    "                    h_val = 1.0 if raw_feedback == 1.0 else -1.0\n",
    "                    human_model.train(state, action, h_val)\n",
    "                    feedback_count += 1\n",
    "            except EOFError:\n",
    "                break\n",
    "            \n",
    "            # Reset display\n",
    "            clear_output(wait=True)\n",
    "            display(fig, display_id='simulation_window')\n",
    "\n",
    "        # Env STEP\n",
    "        # Predict human reward for the CURRENT state/action\n",
    "        predicted_h = human_model.predict(state, action)\n",
    "        \n",
    "        next_obs, env_reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.array(next_obs)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Select next action\n",
    "        next_action = tamer_agent.act(next_state, rounding_size)\n",
    "\n",
    "        # CALCULATE SHAPED REWARD & UPDATE\n",
    "        \"\"\" Add your code here  total_reward= \"\"\"\n",
    "        \n",
    "        tamer_agent.update_q_values(\n",
    "            state, action, total_reward, next_state, next_action, \n",
    "            alpha, gamma, rounding_size, done\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        episode_reward += env_reward\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "\n",
    "    total_reward_per_episode.append(episode_reward)\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Episode {episode+1} finished. Total Env Reward: {episode_reward}\")\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjnis3Ge8woi"
   },
   "source": [
    "Now we plot how the episodic reward is changed across different episodes to visualize the training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "vkvItBJKe7Bu",
    "outputId": "3c945bd3-0ea9-4d59-8544-7e0411aaea12"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_episodes + 1), total_reward_per_episode, color='blue', marker='o')\n",
    "plt.title('Reward Shaping: Progress Across All Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Environmental Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJHyIXFGPVpP"
   },
   "source": [
    "## **Question 2 (TAMER + RL: Q-Augmentation)**\n",
    "1. **utilize human feedback for Q augmentation**: fill in the blank below with your code to update the Q-value using human feedback via **Q augmentation** under section 3.1 of the paper of TAMER + RL\n",
    "\n",
    "In this section, you will implement the second hybrid approach: Q-Augmentation. Unlike Reward Shaping, which modifies the reward signal R, Q-Augmentation combines the reinforcement learning agent's Q-values with the human's reward model to determine the final policy.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Complete the update_q_values method to implement the Q-Augmentation logic as described in section 3.1 of Knox and Stone (2012).\n",
    "\n",
    "2. Observe how the agent uses the internal human model to augment its decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The TAMER Agent (Q-Augmentation Version)\n",
    "\n",
    "**Task:** Complete the update_q_values method. \n",
    "\n",
    "Note that in Q-Augmentation, you typically update the standard Q-table using `R_env` but the action selection (policy) is influenced by `Ĥ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAMERAugmentedAgent:\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_values = {}\n",
    "\n",
    "        self.angle_bins = np.linspace(-0.25, 0.25, 12) \n",
    "        self.vel_bins = np.linspace(-2.0, 2.0, 10) \n",
    "\n",
    "    def get_state_key(self, state):\n",
    "        \"\"\"Maps continuous state to a discrete tuple using bins.\"\"\"\n",
    "        angle_idx = np.digitize(state[2], self.angle_bins)\n",
    "        vel_idx = np.digitize(state[3], self.vel_bins)\n",
    "        return (angle_idx, vel_idx)\n",
    "\n",
    "    def act(self, state, human_model, beta, epsilon=0.1):\n",
    "        state_key = self.get_state_key(state)\n",
    "\n",
    "        # Initialize unseen states\n",
    "        if state_key not in self.q_values:\n",
    "            self.q_values[state_key] = {a: 0.0 for a in range(self.action_size)}\n",
    "            \n",
    "        # Exploration\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "\n",
    "        # Q-Augmentation: Combine Q-value with Scaled Human Prediction\n",
    "        augmented_values = {}\n",
    "        for action in range(self.action_size):\n",
    "            q_rl = self.q_values[state_key][action]\n",
    "            \n",
    "            # Get human prediction\n",
    "            h_hat = human_model.predict(state, action)\n",
    "            h_hat = np.clip(h_hat, -1.0, 1.0) # Clip for safety\n",
    "            \n",
    "            # Combine\n",
    "            augmented_values[action] = q_rl + (beta * h_hat)\n",
    "\n",
    "        return max(augmented_values, key=augmented_values.get)\n",
    "\n",
    "    def update_q_values(self, state, action, env_reward, next_state, next_action, alpha, gamma, done):\n",
    "        state_key = self.get_state_key(state)\n",
    "        next_state_key = self.get_state_key(next_state)\n",
    "\n",
    "        if state_key not in self.q_values:\n",
    "            self.q_values[state_key] = {a: 0.0 for a in range(self.action_size)}\n",
    "        if next_state_key not in self.q_values:\n",
    "            self.q_values[next_state_key] = {a: 0.0 for a in range(self.action_size)}\n",
    "\n",
    "        # TASK: Implement the standard SARSA update using ONLY the environment reward\n",
    "        ''' Add your code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Interactive Training Loop (Q-Augmentation)\n",
    "\n",
    "Observe how the agent behaves. Notice that `env_reward` is used for the update, while the `human_model` is used only for the `act` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha, beta, gamma = 0.1, 5.0, 0.95\n",
    "num_episodes, timeout_threshold = 50, 500\n",
    "feedback_budget, rounding_size = 50, 1\n",
    "\n",
    "# Initialize\n",
    "augmented_agent = TAMERAugmentedAgent(state_size, action_size)\n",
    "human_model = HumanModel()\n",
    "total_reward_per_episode_aug = []\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "display(fig, display_id='final_plot')\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = np.array(obs)\n",
    "    episode_reward = 0\n",
    "    feedback_count = 0\n",
    "    \n",
    "    # Initial action\n",
    "    action = augmented_agent.act(state, human_model, beta, 0.1)\n",
    "\n",
    "    for iteration in range(timeout_threshold):\n",
    "        # Render\n",
    "        image = env.render()\n",
    "        ax.clear()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"Episode: {episode+1} | Step: {iteration}\\n\"\n",
    "                    f\"Agent Action: {actions_dic[action]}\")\n",
    "        ax.axis('off')\n",
    "        update_display(fig, display_id='final_plot')\n",
    "\n",
    "        # Human Feedback\n",
    "        if feedback_count < feedback_budget and random.random() > 0.85:\n",
    "            clear_output(wait=True)\n",
    "            display(fig, display_id='final_plot')\n",
    "            \n",
    "            print(f\"--- FEEDBACK ---\")\n",
    "            print(f\"Pole Angle: {state[2]:.2f} | Agent Chose: {actions_dic[action]}\")\n",
    "            \n",
    "            try:\n",
    "                user_input = input(\"Rate (1=Good, 0=Bad, Enter=Skip): \").strip()\n",
    "                if user_input in [\"0\", \"1\"]:\n",
    "                    val = 1.0 if user_input == \"1\" else -1.0\n",
    "                    human_model.train(state, action, val)\n",
    "                    feedback_count += 1\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            display(fig, display_id='final_plot')\n",
    "\n",
    "        # Env Step\n",
    "        next_obs, env_reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.array(next_obs)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update step\n",
    "        next_action = augmented_agent.act(next_state, human_model, beta, 0.1)\n",
    "        \n",
    "        # Update Q-Values\n",
    "        augmented_agent.update_q_values(\n",
    "            state, action, env_reward, next_state, next_action, \n",
    "            alpha, gamma, done\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        episode_reward += env_reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    total_reward_per_episode_aug.append(episode_reward)\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Episode {episode+1} finished. Reward: {episode_reward}\")\n",
    "\n",
    "plt.close(fig)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsQi50C_VBct"
   },
   "source": [
    "Now again, we plot how the episodic reward is changed across different episodes to visualize the training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "NHy3Pe54S7vP",
    "outputId": "82d0247a-4dcd-452a-aa2b-6a7368d8947f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_episodes + 1), total_reward_per_episode_aug, color='green', marker='o')\n",
    "plt.title('TAMER + Q-Augmentation Performance')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps Survived')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQo5L-2E9-Ao"
   },
   "source": [
    "## **Question 3: Comparison and Analysis**\n",
    "\n",
    "Now that you have implemented both **Reward Shaping** and **Q-Augmentation**, it is time to compare their performance against an **Autonomous Agent** that learns purely from environment rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Autonomous Baseline\n",
    "\n",
    "Run the cell below to train an agent without any human intervention. This agent relies strictly on the environment reward (+1 for every step the pole is balanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha, gamma = 0.2, 0.95\n",
    "num_episodes, timeout_threshold = 50, 500\n",
    "rounding_size = 1\n",
    "\n",
    "# Reset metrics\n",
    "autonomous_agent = TAMERAgent(state_size, action_size)\n",
    "auto_reward_per_episode = []\n",
    "\n",
    "print(\"Training Autonomous Agent (No Human Feedback)...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = np.array(obs)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for iteration in range(timeout_threshold):\n",
    "        action = autonomous_agent.act(state, rounding_size)\n",
    "        next_obs, env_reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.array(next_obs)\n",
    "\n",
    "        # Update\n",
    "        next_action = autonomous_agent.act(next_state, rounding_size)\n",
    "        autonomous_agent.update_q_values(\n",
    "            state, action, env_reward, next_state, next_action, alpha, gamma, rounding_size, True\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += env_reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    auto_reward_per_episode.append(episode_reward)\n",
    "    print(f\"Autonomous Episode {episode+1} finished. Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Performance Visualization\n",
    "\n",
    "Use the cell below to plot the learning curves of all three methods.\n",
    "\n",
    "**NOTE:** Ensure you have the lists from the previous tasks ready. If you followed the naming convention, they will be plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Note: You should have stored results from Q1 and Q2 in separate lists\n",
    "# For this example, we assume they are named accordingly:\n",
    "plt.plot(\n",
    "    range(1, num_episodes + 1),\n",
    "    auto_reward_per_episode, # change here if needed\n",
    "    label='Autonomous (Baseline)',\n",
    "    linestyle='--',\n",
    "    color='gray',\n",
    "    marker='o'\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, num_episodes + 1),\n",
    "    total_reward_per_episode, # change here if needed\n",
    "    label='TAMER + RL (Shaping)',\n",
    "    color='blue',\n",
    "    marker='s'\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, num_episodes + 1),\n",
    "    total_reward_per_episode_aug, # change here if needed\n",
    "    label='TAMER + RL (Augmentation)',\n",
    "    color='green',\n",
    "    marker='^'\n",
    ")\n",
    "\n",
    "plt.title('Learning Curves: Human-in-the-Loop vs. Autonomous')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Environmental Reward (Steps Balanced)')\n",
    "plt.xticks(range(1, num_episodes + 1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reflection Questions\n",
    "\n",
    "**Task:** Answer the questions bellow by editing the markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sample Efficiency: Which method reached a stable performance faster? Why does human feedback typically help more in the first few episodes than in later ones?\n",
    "\n",
    "Your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Method Comparison: Compare Reward Shaping vs. Q-Augmentation. In your experiments, did one lead to more \"stable\" balancing? Refer to the way `Ĥ` is integrated into the math of each method.\n",
    "\n",
    "Your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Human Factors: You provided the feedback yourself. How did your own \"latency\" (the time it took to type) or \"consistency\" (giving different rewards for the same state) likely affect the Decision Tree's training?\n",
    "\n",
    "Your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The \"Model\" Advantage: We used a Decision Tree to model the human. What would happen if we didn't use a model and only updated the agent when you specifically typed a reward? \n",
    "\n",
    "Your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Of Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzsctSwV_dGu"
   },
   "source": [
    "We offered a dummy example to illustrate how the algorithm works. However, a greater amount of human feedback is needed to train your agent. For more details and more advanced implementation, we recommend checking the code from the [original GitHub repository](https://github.com/benibienz/TAMER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoIOFQIjdA1e"
   },
   "source": [
    "# **References**\n",
    "* Knox, W.B., & Stone, P. (2012). Reinforcement learning from simultaneous human and MDP reward. Adaptive Agents and Multi-Agent Systems. [pdf](https://www.cs.cmu.edu/~jeanoh/16-785/papers/knox-aamas2012-tamer.pdf)\n",
    "* Warnell, G., Waytowich, N.R., Lawhern, V.J., & Stone, P. (2017). Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces. ArXiv, abs/1709.10163.[pdf](https://arxiv.org/pdf/1709.10163.pdf)\n",
    "* Arakawa, R., Kobayashi, S., Unno, Y., Tsuboi, Y., & Maeda, S. (2018). DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback. ArXiv, abs/1810.11748. [pdf](https://www.semanticscholar.org/reader/31ed72a18ed8a1008786130af9f1d61761cff4f3)\n",
    "* Karalus, J., & Lindner, F. (2021). Accelerating the Learning of TAMER with Counterfactual Explanations. 2022 IEEE International Conference on Development and Learning (ICDL), 362-368. [pdf](https://www.semanticscholar.org/reader/243abd03e6fa267219d5afd82f387dddc0db26a3)\n",
    "* Knox, W.B., & Stone, P. (2009). Interactively shaping agents via human reinforcement: the TAMER framework. International Conference on Knowledge Capture. [pdf](https://www.cs.utah.edu/~dsbrown/readings/tamer.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
