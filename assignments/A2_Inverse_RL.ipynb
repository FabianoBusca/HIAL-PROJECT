{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_xg0_rwNqRF"
   },
   "source": [
    "- title: Inverse RL\n",
    "- summary: step by step tutorial about Apprenticeship Learning using Inverse Reinforcement Learning\n",
    "- author: Silvia TULLI\n",
    "- feedback and revision: Kim BARAKA, Mohamed CHETOUANI, Muhan HOU\n",
    "- teaching assistant: Márton Bodó, Fabiano Busca\n",
    "- date: 2026-February-March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU4B-xH0N4nX"
   },
   "source": [
    "This notebook contains an excerpt from the **Human-Interactive Robot Learning (HIRL)** educational module.\\\n",
    "For more information check out [our website](https://sites.google.com/view/hirl-education?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fq-05JlN6uh"
   },
   "source": [
    "# **Goals**\n",
    "* Implement Apprenticeship Learning Algorithm\n",
    "* Apply Apprenticeship Learning to simple tasks.\n",
    "* Implement Maximum Conditional Entropy Inverse Reinforcement Learning (MCE-IRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chu7Me810e3b"
   },
   "source": [
    "# **Apprenticeship Learning using Inverse Reinforcement Learning**\n",
    "Apprenticeship Learning is an approach that involves **learning how to perform tasks by observing an expert**. Inverse Reinforcement Learning (IRL) is a specific technique used to **infer the reward function governing an expert’s behavior** based on their observed actions in different states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkHqt-VT4lfY"
   },
   "source": [
    "## **Inverse Reinforcement Learning vs Behavioral Cloning**\n",
    "\n",
    "Unlike Behavioural Cloning, which directly maps observed states to actions through straightforward supervised learning methods, Inverse Reinforcement Learning (IRL) in Apprenticeship Learning delves into inferring the reward functions that underpin expert behaviours. This approach not only seeks to replicate actions but also to **understand and embody the deeper motivations and goals of the expert**. Consequently, this leads to **greater adaptability and the ability to generalise to new situations**, which Behavioural Cloning often lacks due to its more rigid, data-dependent learning structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EOZyMNJ7Aui"
   },
   "source": [
    "# **Environment Setup**\n",
    "In this tutorial we will demonstrate how to solve the CartPole environment from OpenAI Gym using the Inverse Reinforcement Learning. We refer to the implementation described in the paper [Apprenticeship Learning via Inverse Reinforcement Learning](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf). For the sole purpose of demonstration, the expert trajectories are extrapolated from a Q-learning agent, with discretized observable state space. The IRL algorithm learns from these trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQqz61Zi60ou"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]\n",
    "!pip install seals stable-baselines3[extra] imitation\n",
    "# on macOs quotation marks are required\n",
    "# !pip install 'stable-baselines3[extra]'\n",
    "# !pip install seals 'stable-baselines3[extra]' imitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k5ZW9v4FV7Q"
   },
   "source": [
    "# **Load Environment**\n",
    "![CartPole](https://www.gymlibrary.dev/_images/cart_pole.gif)\n",
    "\n",
    "```\n",
    "Action Space: Discrete(2)\n",
    "Observation Space:\n",
    "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38],\n",
    "    [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
    "```\n",
    "\n",
    "For more information about the environment, let's check out the CartPole environment in the documentation: [CartPole](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuPq5kD8FfKE"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "from seals import base_envs\n",
    "from seals.diagnostics.cliff_world import CliffWorldEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "\n",
    "from imitation.algorithms.mce_irl import (\n",
    "    MCEIRL,\n",
    "    mce_occupancy_measures,\n",
    "    mce_partition_fh,\n",
    "    TabularPolicy,\n",
    ")\n",
    "from imitation.data import rollout\n",
    "from imitation.rewards import reward_nets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "from typing import Dict, Tuple, Any, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13YGN7mqFbL7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "nbins = 10\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zv-u-ZFKGw6"
   },
   "source": [
    "# **Generate Expert Trajectories from Q-learning Expert**\n",
    "We define a **Q-learning agent** to compute a policy for this environment, and then extract the expert trajectories based on that policy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYFyh3hBLETo"
   },
   "outputs": [],
   "source": [
    "def max_dict(d: Dict[Any, float]) -> Tuple[Any, float]:\n",
    "    \"\"\"\n",
    "    looking for the action that gives the maximum value for a given state\n",
    "    \"\"\"\n",
    "    max_v = float('-inf')\n",
    "    for key, val in d.items():\n",
    "        if val > max_v:\n",
    "            max_v = val\n",
    "            max_key = key\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMzrJH_ILMA2"
   },
   "outputs": [],
   "source": [
    "def create_bins() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    create bins to discretize the continuous observable state space\n",
    "    \"\"\"\n",
    "    # obs[0] -> cart position --- -4.8 - 4.8\n",
    "    # obs[1] -> cart velocity --- -inf - inf\n",
    "    # obs[2] -> pole angle    --- -41.8 - 41.8\n",
    "    # obs[3] -> pole velocity --- -inf - inf\n",
    "\n",
    "    bins = np.zeros((4,nbins))\n",
    "    bins[0] = np.linspace(-4.8, 4.8, nbins)\n",
    "    bins[1] = np.linspace(-5, 5, nbins)\n",
    "    bins[2] = np.linspace(-.418, .418, nbins)\n",
    "    bins[3] = np.linspace(-5, 5, nbins)\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGbSnb1hLOaU"
   },
   "outputs": [],
   "source": [
    "def assign_bins(observation: np.ndarray, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    discretizing the continuous observation space into state\n",
    "    \"\"\"\n",
    "    state = np.zeros(4)\n",
    "    for i in range(4):\n",
    "        state[i] = np.digitize(observation[i], bins[i])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49HR4-9BLQXl"
   },
   "outputs": [],
   "source": [
    "def get_state_as_string(state: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    encoding the state into string as dictionary\n",
    "    \"\"\"\n",
    "    string_state=''\n",
    "    for e in state:\n",
    "            string_state = string_state+str(int(e)).zfill(2)\n",
    "    return string_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQHMUw6BLR-b"
   },
   "outputs": [],
   "source": [
    "def get_all_states_as_string() -> List[str]:\n",
    "    states = []\n",
    "    for i in range (nbins+1):\n",
    "        for j in range (nbins+1):\n",
    "            for k in range(nbins+1):\n",
    "                for l in range(nbins+1):\n",
    "                    a=str(i).zfill(2)+str(j).zfill(2)+str(k).zfill(2)+str(l).zfill(2)\n",
    "                    states.append(a)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF1U_fVXwD_f"
   },
   "outputs": [],
   "source": [
    "def initialize_Q() -> Dict[str, Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    initialize your Q table\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "\n",
    "    all_states = get_all_states_as_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quGKt3XvxJkU"
   },
   "source": [
    "## **Question 1**\n",
    "1. Fill in the blank space with your code to update the Q-table. The value of $\\alpha$ and $γ$ are already assigned to the variables named ALPHA and GAMMA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4I_7oxo1VM6"
   },
   "outputs": [],
   "source": [
    "def play_one_game(bins: np.ndarray, Q: Dict[str, Dict[int, float]], eps: float = 0.5) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    train 1 episode\n",
    "    \"\"\"\n",
    "    observation, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    cnt = 0 # number of moves in an episode\n",
    "    state = get_state_as_string(assign_bins(observation, bins))\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        cnt += 1\n",
    "\n",
    "        if np.random.uniform() < eps:\n",
    "            act = env.action_space.sample() # epsilon greedy\n",
    "        else:\n",
    "            act = max_dict(Q[state])[0]\n",
    "        observation, reward, terminated, truncated, info  = env.step(act)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if (terminated or truncated) and cnt < 200:\n",
    "            reward = -300\n",
    "\n",
    "        state_new = get_state_as_string(assign_bins(observation, bins))\n",
    "\n",
    "        a1, max_q_s1a1 = max_dict(Q[state_new])\n",
    "\n",
    "        # update the Q-table for Q(s_t, a_t)\n",
    "        ''' Add your code here '''\n",
    "\n",
    "\n",
    "        state, act = state_new, a1\n",
    "\n",
    "    return total_reward, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YubCiBD1lWB"
   },
   "outputs": [],
   "source": [
    "def play_many_games(bins: np.ndarray, N: int = 10000) -> Tuple[List[int], List[float], Dict[str, Dict[int, float]]]:\n",
    "    \"\"\"\n",
    "    train many episodes\n",
    "    \"\"\"\n",
    "    Q = initialize_Q()\n",
    "\n",
    "    length = []\n",
    "    reward = []\n",
    "    for n in range(N):\n",
    "        #eps=0.5/(1+n*10e-3)\n",
    "        eps = 1.0 / np.sqrt(n+1)\n",
    "\n",
    "        episode_reward, episode_length= play_one_game(bins, Q, eps)\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode: %d, Epislon: %.4f, Reward %d\"%(n,eps,episode_reward))\n",
    "        length.append(episode_length)\n",
    "        reward.append(episode_reward)\n",
    "    env.close()\n",
    "    return length, reward, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y59LeW0C1oGZ"
   },
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards: List[float], title: str = 'Running Average', save: bool = False, name: str = 'result') -> None:\n",
    "    \"\"\"\n",
    "    plotting the average reward during training\n",
    "    \"\"\"\n",
    "    fig=plt.figure()\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(totalrewards[max(0, t-100):(t+1)])\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "#     plt.grid()\n",
    "    if save:\n",
    "        plt.savefig(name+'.png',bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47XYsuou50Qk"
   },
   "outputs": [],
   "source": [
    "def plot_running_avg_std(totalrewards: List[float], title: str = 'Running Average and Standard Deviation', save: bool = False, name: str = 'result') -> None:\n",
    "    \"\"\"\n",
    "    Plotting the average and standard deviation of reward during training.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    running_std = np.empty(N)\n",
    "    for t in range(N):\n",
    "        window = totalrewards[max(0, t-100):(t+1)]\n",
    "        running_avg[t] = np.mean(window)\n",
    "        running_std[t] = np.std(window)\n",
    "    plt.plot(running_avg, label='Running Average')\n",
    "    plt.fill_between(range(N), running_avg - running_std, running_avg + running_std, color='b', alpha=0.2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save:\n",
    "        plt.savefig(name + '.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Akbz2EGc1rYn"
   },
   "outputs": [],
   "source": [
    "def play_policy(bins: np.ndarray, Q: Dict[str, Dict[int, float]], N: int = 1000, render: bool = False, delay: float = 0.01) -> List[float]:\n",
    "    \"\"\"\n",
    "    run an environment using a trained policy\n",
    "    \"\"\"\n",
    "\n",
    "    totalReward=[]\n",
    "    steps=[]\n",
    "    for n in range(N):\n",
    "        observation, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episodeReward = 0\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(delay)\n",
    "            state=get_state_as_string(assign_bins(observation, bins))\n",
    "            act=max_dict(Q[state])[0]\n",
    "            observation, reward, terminated, truncated, info = env.step(act)\n",
    "            episodeReward+=reward\n",
    "        totalReward.append(episodeReward)\n",
    "    env.close()\n",
    "    return totalReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ7Wctla2D3C"
   },
   "outputs": [],
   "source": [
    "bins = create_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IRoQ5lW2F3X"
   },
   "outputs": [],
   "source": [
    "episode_lengths, episode_rewards, expert_Q = play_many_games(bins,N=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn71Gl4V5Iex"
   },
   "outputs": [],
   "source": [
    "print(\"export trained expert model...\")\n",
    "filename = 'expert_Q'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(expert_Q,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9oHBUu-6Cyv"
   },
   "outputs": [],
   "source": [
    "plot_running_avg_std(episode_rewards,title=\"Performance: Expert\",save=True,name='Expert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LkSGSTd72K-"
   },
   "outputs": [],
   "source": [
    "expertReward=play_policy(bins,expert_Q,N=10000,render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prQlTQeJ8VUO"
   },
   "outputs": [],
   "source": [
    "plt.hist(expertReward,bins=50)\n",
    "plt.title(\"Reward Distribution\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "unique, counts = np.unique(expertReward, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgpJsz6r8ZT6"
   },
   "outputs": [],
   "source": [
    "expertReward=play_policy(bins,expert_Q,N=1,render=True,delay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgYByCEg9Sr2"
   },
   "source": [
    "# **Inverse Reinforcement Learning**\n",
    "* **Reward Function Replacement**: In the modified Q-learning algorithm, the standard reward signal returned by the environment is discarded. Instead, the algorithm utilizes a custom reward function $R(s,a)=w^T\\phi(s)$. Here:\n",
    "  * $w$ represents the weight vector learned through the IRL algorithm.\n",
    "  * $\\phi(s)$ denotes the observation features or state representation.\n",
    "This adjustment allows the Q-learning agent to learn from the expert's behavior captured in the reward function learned through IRL.\n",
    "* **Observation Features Normalization**: To ensure that observation features fall within the range $[0,1]$ for calculating feature expectations, each feature element is passed through a sigmoid function for mapping. The sigmoid function ensures that the output lies between 0 and 1, facilitating consistent feature scaling across different dimensions of the observation space.\n",
    "$$y=\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hT-iX1mJSaL"
   },
   "outputs": [],
   "source": [
    "def sigmoid(arry: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "  return 1 / (1 + np.exp(-arry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXDKvkPI_Roc"
   },
   "source": [
    "# **IRL Algorithm Using Projection Method**\n",
    "\n",
    "#### Input:\n",
    "- Expert demonstrations: $\\{(s_1, a_1), (s_2, a_2), ..., (s_N, a_N)\\}$, where $s_i$ is the state and $a_i$ is the action taken by the expert in the demonstration.\n",
    "- Feature function: $\\phi: S \\rightarrow \\mathbb{R}^d$, which maps states to a feature space of dimension $d$.\n",
    "\n",
    "#### Output:\n",
    "- Learned reward function parameters: $w$\n",
    "\n",
    "\n",
    "## Algorithm Steps:\n",
    "1. **Initialization**: Start with an initial guess for the reward function parameters $w$.\n",
    "\n",
    "2. **Feature Expectations Computation**:\n",
    "   - For each state $s_i$ in the expert demonstrations:\n",
    "     - Compute the feature vector $\\phi(s_i)$.\n",
    "     - Accumulate the feature expectation: $\\hat{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\phi(s_i)$.\n",
    "\n",
    "3. **Projection onto Feature Expectations**:\n",
    "   - Solve the optimization problem to find the reward function parameters $w:\n",
    "     w^* = \\arg\\min_w \\| \\hat{\\mu} - \\Phi w \\|_2^2$\n",
    "     where:\n",
    "     - $\\Phi$ is a matrix where each row is the feature vector $\\phi(s_i)$.\n",
    "     - $|\\cdot\\|_2$ denotes the Euclidean norm.\n",
    "\n",
    "4. **Normalization**:\n",
    "Normalize the learned reward function parameters \\( w \\) to ensure compatibility with the original feature space:\n",
    "$ w \\leftarrow \\frac{w}{\\| \\hat{\\mu} \\|_2} $\n",
    "\n",
    "5. **Termination Condition**:\n",
    "   - Check for convergence criteria. If satisfied, terminate; otherwise, return to step $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebft8Ht_C4qe"
   },
   "source": [
    "- **Objective**: The goal of the projection method is to find the reward function parameters $w$ that best match the observed feature expectations $\\hat{\\mu}$ of the expert demonstrations.\n",
    "- **Feature Expectations**: The feature function $\\phi$ is used to transform each state into a feature vector. The feature expectations are computed as the average of these feature vectors over all expert demonstrations.\n",
    "- **Projection onto Feature Expectations**: The optimization problem aims to find the reward function parameters $w$ that minimize the squared Euclidean distance between the computed feature expectations and the feature expectations predicted by the reward function $\\Phi w$.\n",
    "- **Normalization**: Normalizing the learned reward function parameters helps ensure that the scale of $w$ matches the scale of the feature expectations $\\hat{\\mu}$, facilitating better convergence and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XX-rNR5x-Wf8"
   },
   "outputs": [],
   "source": [
    "def getFeatureExpectation(Q: Dict[str, Dict[int, float]], N: int = 1000) -> np.ndarray:\n",
    "    observationSum=np.zeros(4)\n",
    "    for i in range(N):\n",
    "        observation, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        cnt=0\n",
    "        while not (terminated or truncated):\n",
    "            state = get_state_as_string(assign_bins(observation, bins))\n",
    "            act = max_dict(Q[state])[0]\n",
    "            observation, reward, terminated, truncated, info =env.step(act)\n",
    "            observation=sigmoid(observation)\n",
    "            observationSum+=(GAMMA**cnt)*observation\n",
    "            cnt+=1\n",
    "    featureExpectation=observationSum/N\n",
    "\n",
    "    print(\"FeatureExpectation: \", featureExpectation)\n",
    "    return featureExpectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHtkabru0GvY"
   },
   "source": [
    "## **Question 2**\n",
    "1. Fill in the blank spaces with your code to:\n",
    "  - recover the reward function based on IRL\n",
    "  - update the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erFbDtDj-Yom"
   },
   "outputs": [],
   "source": [
    "def irl_play_one_game(bins: np.ndarray, weight: np.ndarray, Q: Dict[str, Dict[int, float]], eps: float = 0.5) -> Tuple[float, int]:\n",
    "    observation, _ = env.reset()\n",
    "    # done = False\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    cnt = 0 # number of moves in an episode\n",
    "    state = get_state_as_string(assign_bins(observation, bins))\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        cnt += 1\n",
    "\n",
    "        if np.random.uniform() < eps:\n",
    "            act = env.action_space.sample() # epsilon greedy\n",
    "        else:\n",
    "            act = max_dict(Q[state])[0]\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(act)\n",
    "\n",
    "        #encode observations into state\n",
    "        state_new = get_state_as_string(assign_bins(observation, bins))\n",
    "\n",
    "        #map observations to 0 and 1\n",
    "        observation=sigmoid(observation)\n",
    "\n",
    "        #discard the simulation reward, and use the reward function found from irl algorithm\n",
    "        ''' Add your code here '''\n",
    "\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if (terminated or truncated) and cnt < 200:\n",
    "            reward = -1\n",
    "\n",
    "\n",
    "        a1, max_q_s1a1 = max_dict(Q[state_new])\n",
    "\n",
    "        # update the Q-table\n",
    "        ''' Add your code here '''\n",
    "\n",
    "\n",
    "        state, act = state_new, a1\n",
    "\n",
    "    return total_reward, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDMeKFuY-al3"
   },
   "outputs": [],
   "source": [
    "def irl_play_many_games(bins: np.ndarray, weight: np.ndarray, N: int = 10000) -> Tuple[List[int], List[float], Dict[str, Dict[int, float]]]:\n",
    "    Q = initialize_Q()\n",
    "    length = []\n",
    "    reward = []\n",
    "    for n in range(N):\n",
    "        eps = 1.0 / np.sqrt(n+1)\n",
    "\n",
    "        episode_reward, episode_length= irl_play_one_game(bins, weight,Q,eps)\n",
    "\n",
    "        length.append(episode_length)\n",
    "        reward.append(episode_reward)\n",
    "    print(\"Avg Length %d\"%(np.average(length)))\n",
    "    print(\"standard deviation %d\"%(np.std(length)))\n",
    "    return length, reward, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYiYhlWY-r7X"
   },
   "outputs": [],
   "source": [
    "expertExpectation=getFeatureExpectation(expert_Q,N=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEfG6TpMC-Nr"
   },
   "outputs": [],
   "source": [
    "# either terminate with margin or iteration\n",
    "epislon=0.00002\n",
    "N=10\n",
    "\n",
    "weight=[]\n",
    "feature_expectation=[]\n",
    "feature_expectation_hat=[]\n",
    "learnedQ=[]\n",
    "margin=[]\n",
    "avgEpisodeLength=[]\n",
    "\n",
    "for i in range(N):\n",
    "    print(\"Iteration: \",i)\n",
    "    if i==0: # step1, initialization\n",
    "        initialQ=initialize_Q() # give random initial policy\n",
    "        feature_expectation.append(getFeatureExpectation(initialQ))\n",
    "        print(\"expert feature Expectation: \", expertExpectation)\n",
    "        learnedQ.append(initialQ) # put in the initial policy\n",
    "        weight.append(np.zeros(4)) # put in a dummy weight\n",
    "        margin.append(1) # put in a dummy margin\n",
    "    else: # first iter of step 2\n",
    "        if i==1:\n",
    "            feature_expectation_hat.append(feature_expectation[i - 1])\n",
    "            weight.append(expertExpectation-feature_expectation[i-1])\n",
    "            margin.append(norm((expertExpectation - feature_expectation_hat[i - 1]), 2))\n",
    "\n",
    "            print(\"margin: \",margin[i])\n",
    "            print(\"weight: \",weight[i])\n",
    "\n",
    "        else: # iter 2 and on of step 2\n",
    "            A=feature_expectation_hat[i - 2]\n",
    "            B=feature_expectation[i-1]-A\n",
    "            C= expertExpectation - feature_expectation_hat[i - 2]\n",
    "            feature_expectation_hat.append(A + (np.dot(B, C) / np.dot(B, B)) * (B))\n",
    "\n",
    "            weight.append(expertExpectation - feature_expectation_hat[i - 1])\n",
    "            margin.append(norm((expertExpectation - feature_expectation_hat[i - 1]), 2))\n",
    "\n",
    "            print(\"margin: \",margin[i])\n",
    "            print(\"weight: \",weight[i])\n",
    "\n",
    "        # step3,terminate condition\n",
    "        if (margin[i]<=epislon):\n",
    "            break\n",
    "\n",
    "        # step4\n",
    "        episode_lengths, episode_rewards, learnedQ_i= irl_play_many_games(bins,weight[i])\n",
    "        learnedQ.append(learnedQ_i)\n",
    "        avgEpisodeLength.append(episode_lengths)\n",
    "        # step5\n",
    "        feature_expectation.append(getFeatureExpectation(learnedQ[i]))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "print(\"export trained IRL model...\")\n",
    "filename = 'learnedQ'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(learnedQ,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HvqdivUM_Vh"
   },
   "outputs": [],
   "source": [
    "# showing the performance of each student\n",
    "for i in range(0,len(avgEpisodeLength)):\n",
    "    title=\"Performance: Student_\"+str(i+1)\n",
    "    plot_running_avg_std(avgEpisodeLength[i],title=title,save=True,name=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XroZ-B9kNkOX"
   },
   "outputs": [],
   "source": [
    "# Plotting Convergence Rate\n",
    "plt.plot(margin)\n",
    "plt.title(\"L2 Policy Error\")\n",
    "plt.xlabel(\"Student Number\")\n",
    "plt.ylabel(\"Squared Error of Features\")\n",
    "plt.savefig(\"policy_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5QuhWIvNo5_"
   },
   "outputs": [],
   "source": [
    "# showing the performance of each student relative to the performance of the expert\n",
    "iteration=[]\n",
    "relativePerformance=[]\n",
    "studentPerformance=[]\n",
    "\n",
    "for i in avgEpisodeLength:\n",
    "    studentPerformance.append(np.average(i))\n",
    "\n",
    "for i in range(np.size(studentPerformance)):\n",
    "    iteration.append(i)\n",
    "    relativePerformance.append(studentPerformance[i])\n",
    "\n",
    "plt.plot(iteration,relativePerformance)\n",
    "plt.xlabel(\"Student Number\")\n",
    "plt.ylabel(\"Episode Length\")\n",
    "plt.title(\"Average Episode Reward\")\n",
    "# plt.grid()\n",
    "plt.savefig(\"studentRewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFuQ3MaFcJGU"
   },
   "source": [
    "# **Maximum Conditional Entropy Inverse Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KGyjV-1Q1hj"
   },
   "source": [
    "In this tutorial we are exploring the **Cliffworld environment** (`CliffWorldEnv`), a [partially observable Markov decision process (POMDP)](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#:~:text=A%20partially%20observable%20Markov%20decision,directly%20observe%20the%20underlying%20state.), to identify the optimal policy within a table-based setup that features a predefined reward function. To achieve this, we first solve for the optimal policy in the Cliffworld scenario. We then produce a series of demonstrations based on this optimal policy and apply Maximum Causal Entropy Inverse Reinforcement Learning (MCE IRL) to derive an approximation of the actual reward function. By having access to the true reward function in this example, we are able to perform a direct comparison between the learned reward and the ground-truth reward, enhancing our understanding of the effectiveness of our learning approach.\n",
    "\n",
    "![cliffword](https://www.bpesquet.fr/mlkatas/_images/cliff_map.png)\n",
    "\n",
    "In Cliffworld, the environment provides \"observations\" that include both partial direct observations and the full hidden state of the environment. To simplify the process of determining the optimal policy, we employ the `DictExtractWrapper` to isolate only the hidden states from these observations. This transformation effectively converts the environment from a partially observable to a fully observable Markov decision process (MDP), thereby facilitating the computation of the optimal policy. This approach not only simplifies the computational complexity but also allows us to focus on the efficiency of the learning algorithms in a controlled setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvdV20GGO-i0"
   },
   "outputs": [],
   "source": [
    "env_creator = partial(CliffWorldEnv, height=4, horizon=40, width=7, use_xy_obs=True)\n",
    "env_single = env_creator()\n",
    "\n",
    "state_env_creator = lambda: base_envs.ExposePOMDPStateWrapper(env_creator())\n",
    "\n",
    "# This is just a vectorized environment because `generate_trajectories` expects one\n",
    "state_venv = DummyVecEnv([state_env_creator] * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KQiba6GRJBE"
   },
   "source": [
    "Then we derive an expert policy using Bellman backups. We analytically compute the occupancy measures, and also sample some expert trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhdCWf2gP79W"
   },
   "outputs": [],
   "source": [
    "_, _, pi = mce_partition_fh(env_single)\n",
    "\n",
    "_, om = mce_occupancy_measures(env_single, pi=pi)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "expert = TabularPolicy(\n",
    "    state_space=env_single.state_space,\n",
    "    action_space=env_single.action_space,\n",
    "    pi=pi,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "expert_trajs = rollout.generate_trajectories(\n",
    "    policy=expert,\n",
    "    venv=state_venv,\n",
    "    sample_until=rollout.make_min_timesteps(5000),\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "print(\"Expert stats: \", rollout.rollout_stats(expert_trajs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsdH3ArwRiO2"
   },
   "source": [
    "# **Training the reward function**\n",
    "The true reward here is not linear in the reduced feature space (i.e, coordinates). Finding an appropriate linear reward is impossible, but an MLP should Just Work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvB1HIWaP-05"
   },
   "outputs": [],
   "source": [
    "def train_mce_irl(demos: Any, hidden_sizes: List[int], lr: float = 0.01, **kwargs: Any) -> Any:\n",
    "    reward_net = reward_nets.BasicRewardNet(\n",
    "        env_single.observation_space,\n",
    "        env_single.action_space,\n",
    "        hid_sizes=hidden_sizes,\n",
    "        use_action=False,\n",
    "        use_done=False,\n",
    "        use_next_state=False,\n",
    "    )\n",
    "\n",
    "    mce_irl = MCEIRL(\n",
    "        demos,\n",
    "        env_single,\n",
    "        reward_net,\n",
    "        log_interval=250,\n",
    "        optimizer_kwargs=dict(lr=lr),\n",
    "        rng=rng,\n",
    "    )\n",
    "    occ_measure = mce_irl.train(**kwargs)\n",
    "\n",
    "    imitation_trajs = rollout.generate_trajectories(\n",
    "        policy=mce_irl.policy,\n",
    "        venv=state_venv,\n",
    "        sample_until=rollout.make_min_timesteps(5000),\n",
    "        rng=rng,\n",
    "    )\n",
    "    print(\"Imitation stats: \", rollout.rollout_stats(imitation_trajs))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    env_single.draw_value_vec(occ_measure)\n",
    "    plt.title(\"Occupancy for learned reward\")\n",
    "    plt.xlabel(\"Gridworld x-coordinate\")\n",
    "    plt.ylabel(\"Gridworld y-coordinate\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    _, true_occ_measure = mce_occupancy_measures(env_single)\n",
    "    env_single.draw_value_vec(true_occ_measure)\n",
    "    plt.title(\"Occupancy for true reward\")\n",
    "    plt.xlabel(\"Gridworld x-coordinate\")\n",
    "    plt.ylabel(\"Gridworld y-coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    env_single.draw_value_vec(\n",
    "        reward_net(th.as_tensor(env_single.observation_matrix), None, None, None)\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "    plt.title(\"Learned reward\")\n",
    "    plt.xlabel(\"Gridworld x-coordinate\")\n",
    "    plt.ylabel(\"Gridworld y-coordinate\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    env_single.draw_value_vec(env_single.reward_matrix)\n",
    "    plt.title(\"True reward\")\n",
    "    plt.xlabel(\"Gridworld x-coordinate\")\n",
    "    plt.ylabel(\"Gridworld y-coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "    return mce_irl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfs6qboWRuh5"
   },
   "source": [
    "As you can see, a linear reward model cannot fit the data. Even though we're training the model on analytically computed occupancy measures for the optimal policy, the resulting reward and occupancy frequencies diverge sharply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvL1Kvj_Rwm2"
   },
   "outputs": [],
   "source": [
    "train_mce_irl(om, hidden_sizes=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL-VEoTwRyZi"
   },
   "source": [
    "Now, let's try using a very simple nonlinear reward model: an MLP with a single hidden layer. We first train it on the analytically computed occupancy measures. This should give a very precise result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrwle7slRz3S"
   },
   "outputs": [],
   "source": [
    "train_mce_irl(om, hidden_sizes=[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp5RCV3BR1Vq"
   },
   "source": [
    "Then we train it on trajectories sampled from the expert. This gives a stochastic approximation to occupancy measure, so performance is a little worse. Using more expert trajectories should improve performance -- try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1VLZfur7zcw"
   },
   "source": [
    "## **Question 3**\n",
    "1. fill in the blank with your code to train an MCE-IRL model using both 10 expert demonstrations and an MLP with one hidden layer of 256 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrbMKHW_R3R6"
   },
   "outputs": [],
   "source": [
    "# use the first 10 demonstrations from the expert_trajs and a 256-sized hidden layer to train IRL\n",
    "''' Add your code here '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEgxAEZjW8zJ"
   },
   "source": [
    "## **Question 4**\n",
    "1. With the extra expert trajectories, is the learned reward function more similar to the true reward function?\n",
    "2. With the extra expert trajectories, is the occupancy measure more similar to the true one?\n",
    "3. Explain why ending up in the cell that is just below the top rightmost one was considered almost as good as the top rightmost cell by the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhVNfc8zOwy-"
   },
   "source": [
    "# **References**\n",
    "* Abbeel, P., & Ng, A. Y. (2004, July). Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning (p. 1).\n",
    "* Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008, July). Maximum entropy inverse reinforcement learning. In Aaai (Vol. 8, pp. 1433-1438)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
